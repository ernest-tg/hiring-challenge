{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-trivial model\n",
    "\n",
    "Finally, we make an attempt at classifying videos.\n",
    "\n",
    "We train an image classifier. Once it is trained, we will classify videos this way: classify every frame of the video, and average the probas to get the classification probabilities for the whole video.\n",
    "\n",
    "* This has the advantage of giving us ~10k (input, groundtruth) pairs, compared to ~200 for an end-to-end video classifier.\n",
    "* Image classifiers are easier to visualize and debug.\n",
    "* There is more research being done on images, which leads to better architectures/models despite the theoretical disadvantage of the limitation. For instance, __[the CoCa video classifier](https://paperswithcode.com/paper/coca-contrastive-captioners-are-image-text)__ (which is based on an image-model, with late fusion of information) is SOTA for some video datasets, and among the best models for others.\n",
    "* It is easier to use the image models, since they are packaged with the main deep learning frameworks.\n",
    "\n",
    "My initial plan was to use this as a baseline and then train a linear layer on top of a __[MTV model](https://paperswithcode.com/paper/multiview-transformers-for-video-recognition)__ . However, the problem I got with the image-based classifier was not that its accuracy was too low. It was suspiciously too high. This pushed me to try fixing the dataset either. But, even after:\n",
    "* Writing a resizing transformation which does not show the difference in height between fights and non-fight.\n",
    "* Applying an extreme color augmentation until tiny models do not do better than chance.\n",
    "* Manually annotating videos to avoid correlations between train and validation datasets.\n",
    "\n",
    "The validation accuracy we get is close to 100%. There are probably other Clever Hans effects we did not get rid off. But, at this point, it is outside the scope of this challenge. We would recommend spending more time on aquiring more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from fight_classifier import DATASET_DIR, PROJECT_DIR\n",
    "\n",
    "frames_dir = DATASET_DIR / 'raw_frames/'\n",
    "videos_dir = DATASET_DIR / 'Peliculas/'\n",
    "\n",
    "videos_df = pd.read_csv(videos_dir / 'videos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthieu/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/matthieu/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/matthieu/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageClassifierModule(\n",
       "  (classifier): ProjFromFeatures(\n",
       "    (base_model): MobileNetV3(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (12): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (15): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (16): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=960, out_features=1280, bias=True)\n",
       "        (1): Hardswish()\n",
       "        (2): Dropout(p=0.2, inplace=True)\n",
       "        (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (proj_layer): Linear(in_features=960, out_features=2, bias=True)\n",
       "    (feature_extractor): MobileNetV3(\n",
       "      (features): Module(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (12): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (15): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (16): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "  )\n",
       "  (train_precision): Precision()\n",
       "  (train_recall): Recall()\n",
       "  (train_f1): F1Score()\n",
       "  (train_pr_curve): PrecisionRecallCurve()\n",
       "  (train_confusion_matrix): ConfusionMatrix()\n",
       "  (val_precision): Precision()\n",
       "  (val_recall): Recall()\n",
       "  (val_f1): F1Score()\n",
       "  (val_pr_curve): PrecisionRecallCurve()\n",
       "  (val_confusion_matrix): ConfusionMatrix()\n",
       "  (test_precision): Precision()\n",
       "  (test_recall): Recall()\n",
       "  (test_f1): F1Score()\n",
       "  (test_pr_curve): PrecisionRecallCurve()\n",
       "  (test_confusion_matrix): ConfusionMatrix()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchvision.models import MobileNet_V3_Large_Weights\n",
    "\n",
    "from fight_classifier.data.image_dataset import ImageDataModule\n",
    "from fight_classifier.model.image_based_model import \\\n",
    "    ProjFromFeatures\n",
    "from fight_classifier.torch_module.image_classifier import \\\n",
    "    ImageClassifierModule\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "SPLIT_COHERENCE_COL = 'fine_category'\n",
    "\n",
    "frames_df = pd.read_csv(str(frames_dir / 'frames.csv'))\n",
    "\n",
    "# We base our pre-processing on Mobilenet because it's the\n",
    "# base of the model we will train in the next notebook.\n",
    "base_model_weights = MobileNet_V3_Large_Weights.DEFAULT\n",
    "preprocess = base_model_weights.transforms()\n",
    "preprocess_kwargs = {\n",
    "    'resize_size': preprocess.resize_size[0],\n",
    "    'crop_size': preprocess.crop_size[0],\n",
    "    'mean': preprocess.mean,\n",
    "    'std': preprocess.std,\n",
    "}\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=str(PROJECT_DIR),\n",
    "    max_epochs=10,\n",
    ")\n",
    "\n",
    "\n",
    "image_data_module = ImageDataModule(\n",
    "    image_df=frames_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    preprocess_kwargs=preprocess_kwargs,\n",
    "    split_coherence_col=SPLIT_COHERENCE_COL,\n",
    "    image_augmentation=False)\n",
    "image_data_module.setup()\n",
    "classifier = ProjFromFeatures(n_classes=2)\n",
    "classif_module = ImageClassifierModule(classifier=classifier)\n",
    "\n",
    "# Instead of training the model, we can load it from the checkpoint\n",
    "\n",
    "# trainer.fit(\n",
    "#    model=classif_module,\n",
    "#    datamodule=image_data_module)\n",
    "\n",
    "classif_module.load_from_checkpoint(\n",
    "    str(PROJECT_DIR / 'checkpoints/epoch=9-step=2110.ckpt'),\n",
    "    classifier=classifier,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From images to videos\n",
    "\n",
    "We use `ImageBasedVideoClassifier` to classify videos: we run the image classifier on every other image (to save computation time) and average them to get the video classification. Logically, since the image classifier's validation performance is almost perfect, the video classification is also almost perfect on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                                                                                                     | 1/197 [00:02<06:53,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▋                                                                                                                                                                    | 2/197 [00:04<06:45,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|██▌                                                                                                                                                                   | 3/197 [00:06<06:44,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|███▎                                                                                                                                                                  | 4/197 [00:08<06:31,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|████▏                                                                                                                                                                 | 5/197 [00:10<06:32,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█████                                                                                                                                                                 | 6/197 [00:12<06:33,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█████▉                                                                                                                                                                | 7/197 [00:14<06:26,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██████▋                                                                                                                                                               | 8/197 [00:16<06:19,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███████▌                                                                                                                                                              | 9/197 [00:18<06:22,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|████████▍                                                                                                                                                            | 10/197 [00:20<06:20,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████████▏                                                                                                                                                           | 11/197 [00:22<06:14,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██████████                                                                                                                                                           | 12/197 [00:24<06:10,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|██████████▉                                                                                                                                                          | 13/197 [00:26<06:11,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███████████▋                                                                                                                                                         | 14/197 [00:28<06:19,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|████████████▌                                                                                                                                                        | 15/197 [00:30<06:22,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|█████████████▍                                                                                                                                                       | 16/197 [00:33<06:27,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|██████████████▏                                                                                                                                                      | 17/197 [00:35<06:30,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███████████████                                                                                                                                                      | 18/197 [00:37<06:32,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███████████████▉                                                                                                                                                     | 19/197 [00:39<06:28,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████████████▊                                                                                                                                                    | 20/197 [00:41<06:19,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████████████████▌                                                                                                                                                   | 21/197 [00:43<06:09,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|██████████████████▍                                                                                                                                                  | 22/197 [00:45<06:05,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████████████████▎                                                                                                                                                 | 23/197 [00:47<05:58,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|████████████████████                                                                                                                                                 | 24/197 [00:49<05:54,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████████████████████▊                                                                                                                                               | 26/197 [00:54<06:11,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████████████████████▌                                                                                                                                              | 27/197 [00:56<06:07,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|███████████████████████▍                                                                                                                                             | 28/197 [00:58<06:03,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████████████▏                                                                                                                                           | 30/197 [01:03<06:29,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████████████████████▉                                                                                                                                           | 31/197 [01:06<06:33,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████████████████████████▊                                                                                                                                          | 32/197 [01:08<06:33,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████████████████████████▋                                                                                                                                         | 33/197 [01:10<06:33,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|████████████████████████████▍                                                                                                                                        | 34/197 [01:13<06:13,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█████████████████████████████▎                                                                                                                                       | 35/197 [01:15<06:18,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████████████████████████▏                                                                                                                                      | 36/197 [01:17<06:19,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|██████████████████████████████▉                                                                                                                                      | 37/197 [01:19<06:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████████████████████▊                                                                                                                                     | 38/197 [01:21<05:47,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████████████████████▋                                                                                                                                    | 39/197 [01:23<05:34,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████████████████████████████▌                                                                                                                                   | 40/197 [01:25<05:25,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|██████████████████████████████████▎                                                                                                                                  | 41/197 [01:27<05:20,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|███████████████████████████████████▏                                                                                                                                 | 42/197 [01:29<05:17,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|████████████████████████████████████                                                                                                                                 | 43/197 [01:31<05:13,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|████████████████████████████████████▊                                                                                                                                | 44/197 [01:33<05:08,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([51, 3, 224, 224])\n",
      "before  torch.Size([1, 51, 3, 224, 224])\n",
      "after  torch.Size([1, 26, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████████████████████████▌                                                                                                                              | 46/197 [01:38<05:41,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([51, 3, 224, 224])\n",
      "before  torch.Size([1, 51, 3, 224, 224])\n",
      "after  torch.Size([1, 26, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([51, 3, 224, 224])\n",
      "before  torch.Size([1, 51, 3, 224, 224])\n",
      "after  torch.Size([1, 26, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|████████████████████████████████████████▏                                                                                                                            | 48/197 [01:43<05:46,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([51, 3, 224, 224])\n",
      "before  torch.Size([1, 51, 3, 224, 224])\n",
      "after  torch.Size([1, 26, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████████████████████████                                                                                                                            | 49/197 [01:46<05:48,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([51, 3, 224, 224])\n",
      "before  torch.Size([1, 51, 3, 224, 224])\n",
      "after  torch.Size([1, 26, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████████████████████████▉                                                                                                                           | 50/197 [01:48<05:49,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██████████████████████████████████████████▋                                                                                                                          | 51/197 [01:50<05:51,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|███████████████████████████████████████████▌                                                                                                                         | 52/197 [01:53<05:50,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|████████████████████████████████████████████▍                                                                                                                        | 53/197 [01:55<05:48,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|█████████████████████████████████████████████▏                                                                                                                       | 54/197 [01:58<05:45,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██████████████████████████████████████████████                                                                                                                       | 55/197 [02:00<05:41,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██████████████████████████████████████████████▉                                                                                                                      | 56/197 [02:03<05:40,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████████████████████████████████████████▌                                                                                                                    | 58/197 [02:07<05:19,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████████████████████████████████████████▍                                                                                                                   | 59/197 [02:09<05:08,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████████████████████████████████████▎                                                                                                                  | 60/197 [02:11<04:58,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████████████████████████████████                                                                                                                  | 61/197 [02:13<04:51,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███████████████████████████████████████████████████▉                                                                                                                 | 62/197 [02:15<04:42,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|████████████████████████████████████████████████████▊                                                                                                                | 63/197 [02:17<04:40,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████████████████████████████████████████████▌                                                                                                               | 64/197 [02:19<04:37,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████████████████████████████████████████████▍                                                                                                              | 65/197 [02:21<04:29,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|███████████████████████████████████████████████████████▎                                                                                                             | 66/197 [02:23<04:29,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|████████████████████████████████████████████████████████                                                                                                             | 67/197 [02:25<04:25,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|████████████████████████████████████████████████████████▉                                                                                                            | 68/197 [02:27<04:21,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████████████████████████████████████████▊                                                                                                           | 69/197 [02:29<04:20,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|██████████████████████████████████████████████████████████▋                                                                                                          | 70/197 [02:31<04:13,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████████████████████████████████████████████████▍                                                                                                         | 71/197 [02:33<04:16,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([42, 3, 224, 224])\n",
      "before  torch.Size([1, 42, 3, 224, 224])\n",
      "after  torch.Size([1, 21, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|████████████████████████████████████████████████████████████▎                                                                                                        | 72/197 [02:36<04:24,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|█████████████████████████████████████████████████████████████▏                                                                                                       | 73/197 [02:38<04:27,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|█████████████████████████████████████████████████████████████▉                                                                                                       | 74/197 [02:41<04:41,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████████████████████████████████████████████████████████▊                                                                                                      | 75/197 [02:43<04:50,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███████████████████████████████████████████████████████████████▋                                                                                                     | 76/197 [02:46<04:54,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|████████████████████████████████████████████████████████████████▍                                                                                                    | 77/197 [02:48<04:56,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████████████████████████████████████▎                                                                                                   | 78/197 [02:51<05:03,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|██████████████████████████████████████████████████████████████████▏                                                                                                  | 79/197 [02:54<05:17,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████████████████████████████████████████                                                                                                  | 80/197 [02:57<05:28,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|███████████████████████████████████████████████████████████████████▊                                                                                                 | 81/197 [03:00<05:34,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████████████████████████████████████████████████████████████████████▋                                                                                                | 82/197 [03:03<05:34,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|█████████████████████████████████████████████████████████████████████▌                                                                                               | 83/197 [03:06<05:43,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|██████████████████████████████████████████████████████████████████████▎                                                                                              | 84/197 [03:09<05:41,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|███████████████████████████████████████████████████████████████████████▏                                                                                             | 85/197 [03:12<05:33,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████████████████████████████████████████                                                                                             | 86/197 [03:15<05:23,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████████████████████████████████████████▊                                                                                            | 87/197 [03:18<05:20,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|█████████████████████████████████████████████████████████████████████████▋                                                                                           | 88/197 [03:21<05:18,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████████████████████████████████████████████████████████████▌                                                                                          | 89/197 [03:24<05:14,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████████████████████████████████████████████████████████████▍                                                                                         | 90/197 [03:27<05:12,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████████████████████████████████████████████████████████████████████████████▏                                                                                        | 91/197 [03:30<05:13,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|█████████████████████████████████████████████████████████████████████████████                                                                                        | 92/197 [03:33<05:12,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|█████████████████████████████████████████████████████████████████████████████▉                                                                                       | 93/197 [03:36<05:14,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|██████████████████████████████████████████████████████████████████████████████▋                                                                                      | 94/197 [03:39<05:11,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████████████████████████████████████████████▌                                                                                     | 95/197 [03:42<05:07,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████████████████████████████████████████████████████████████████▍                                                                                    | 96/197 [03:45<05:03,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████████████████████████████████████████████████████████████████▏                                                                                   | 97/197 [03:48<04:57,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([60, 3, 224, 224])\n",
      "before  torch.Size([1, 60, 3, 224, 224])\n",
      "after  torch.Size([1, 30, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████▉                                                                                  | 99/197 [03:53<04:28,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|███████████████████████████████████████████████████████████████████████████████████▏                                                                                | 100/197 [03:55<04:13,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|████████████████████████████████████████████████████████████████████████████████████                                                                                | 101/197 [03:58<04:03,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|████████████████████████████████████████████████████████████████████████████████████▉                                                                               | 102/197 [04:00<03:59,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████████████████████████████████████████████████████████████████████████████████████▋                                                                              | 103/197 [04:03<03:56,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|██████████████████████████████████████████████████████████████████████████████████████▌                                                                             | 104/197 [04:05<03:53,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████████████████████████████████████████████████████████████▍                                                                            | 105/197 [04:07<03:45,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|████████████████████████████████████████████████████████████████████████████████████████▏                                                                           | 106/197 [04:10<03:38,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████████████████████████████████████████████████████████████████████████████████████████                                                                           | 107/197 [04:12<03:34,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████████████████████████████████████████████████████████████▉                                                                          | 108/197 [04:14<03:30,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|██████████████████████████████████████████████████████████████████████████████████████████▋                                                                         | 109/197 [04:17<03:28,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 110/197 [04:19<03:26,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████████████████████████████████████████████████████████████████████████▍                                                                       | 111/197 [04:22<03:26,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████████████████████████████████████████████████████████████████████████████████████████████▏                                                                      | 112/197 [04:24<03:29,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████████████████████████████████████████████████████████████████                                                                      | 113/197 [04:27<03:31,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                                    | 115/197 [04:32<03:31,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                   | 116/197 [04:34<03:23,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                  | 117/197 [04:37<03:19,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                 | 118/197 [04:39<03:13,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████                                                                 | 119/197 [04:42<03:09,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                               | 121/197 [04:47<03:04,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                              | 122/197 [04:49<03:02,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 123/197 [04:51<02:59,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                            | 124/197 [04:54<03:02,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|████████████████████████████████████████████████████████████████████████████████████████████████████████                                                            | 125/197 [04:57<03:04,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                          | 127/197 [05:02<02:53,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 128/197 [05:04<02:47,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 129/197 [05:06<02:43,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                       | 130/197 [05:09<02:44,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                       | 131/197 [05:11<02:45,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                     | 133/197 [05:16<02:35,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                   | 135/197 [05:21<02:29,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 136/197 [05:23<02:27,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                 | 138/197 [05:28<02:23,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                               | 140/197 [05:33<02:20,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                              | 141/197 [05:36<02:16,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 142/197 [05:38<02:16,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                             | 143/197 [05:41<02:11,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 144/197 [05:43<02:06,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                           | 145/197 [05:45<02:07,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 146/197 [05:48<02:06,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 147/197 [05:51<02:05,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 148/197 [05:53<02:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                       | 150/197 [05:58<01:57,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                     | 152/197 [06:03<01:52,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                    | 153/197 [06:06<01:51,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                   | 154/197 [06:08<01:50,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 156/197 [06:13<01:43,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                 | 157/197 [06:16<01:40,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                               | 159/197 [06:21<01:35,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                              | 160/197 [06:23<01:34,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                              | 161/197 [06:26<01:32,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 162/197 [06:29<01:30,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                           | 164/197 [06:33<01:23,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                          | 165/197 [06:36<01:19,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                         | 167/197 [06:41<01:14,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                        | 168/197 [06:43<01:12,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 169/197 [06:46<01:10,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                      | 170/197 [06:48<01:08,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 172/197 [06:54<01:03,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 173/197 [06:56<01:01,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                  | 175/197 [07:01<00:55,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 177/197 [07:06<00:50,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏               | 178/197 [07:09<00:48,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████               | 179/197 [07:11<00:44,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋             | 181/197 [07:17<00:41,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌            | 182/197 [07:19<00:38,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 184/197 [07:24<00:33,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████          | 185/197 [07:27<00:30,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 186/197 [07:29<00:28,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 188/197 [07:35<00:23,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 189/197 [07:37<00:20,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏     | 190/197 [07:39<00:17,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████     | 191/197 [07:42<00:15,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 192/197 [07:44<00:12,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 194/197 [07:49<00:07,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 195/197 [07:52<00:04,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n",
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 197/197 [07:57<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_frames.shape  torch.Size([50, 3, 224, 224])\n",
      "before  torch.Size([1, 50, 3, 224, 224])\n",
      "after  torch.Size([1, 25, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import skvideo.io\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from fight_classifier.model.image_based_model import ImageBasedVideoClassifier\n",
    "\n",
    "classifier.eval()\n",
    "video_classifier = ImageBasedVideoClassifier(\n",
    "    image_classifier=classifier)\n",
    "\n",
    "predictions_probas = []\n",
    "predictions = []\n",
    "groundtruths = []\n",
    "\n",
    "\n",
    "for _, video_row in tqdm.tqdm(videos_df.iterrows(), total=len(videos_df)):\n",
    "    video_path = str(DATASET_DIR / video_row.video_path)\n",
    "    video_fhwc = skvideo.io.vread(video_path)\n",
    "    with torch.no_grad():\n",
    "        frames_bhwc = torch.Tensor([\n",
    "            image_data_module.train_dataset.preprocess_image(image_np)[1]\n",
    "            for image_np in video_fhwc\n",
    "        ])\n",
    "        frames_bchw = torch.permute(frames_bhwc, (0, 3, 1, 2))\n",
    "        print(\"normalized_frames.shape \", frames_bchw.shape)\n",
    "        video_proba = video_classifier(frames_bchw[None]).numpy()\n",
    "        groundtruths.append(video_row.is_fight)\n",
    "        predictions_probas.append(video_proba)\n",
    "        predictions.append(np.argmax(video_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of video classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.98\n",
      "Recall = 1.00\n",
      "F1-score = 0.99\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fight_classifier.visualization import imshow_chw\n",
    "from fight_classifier.visualization.metrics import plot_confusion_matrix\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(\n",
    "    y_true=groundtruths, y_pred=predictions)\n",
    "precision = confusion_matrix[1,1] / confusion_matrix[:,1].sum()\n",
    "recall = confusion_matrix[1,1] / confusion_matrix[1,:].sum()\n",
    "f1_score = 2 * precision * recall / (precision+recall)\n",
    "print(\n",
    "    f\"Precision = {precision:.2f}\\n\"\n",
    "    f\"Recall = {recall:.2f}\\n\"\n",
    "    f\"F1-score = {f1_score:.2f}\\n\"\n",
    ")\n",
    "cm_viz_chw = plot_confusion_matrix(\n",
    "    cm=confusion_matrix, class_names=['no_fight', 'fight'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_axis_off()\n",
    "imshow_chw(img_chw=cm_viz_chw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation map\n",
    "\n",
    "I thought I would show some activation maps of the image model, thanks to Torch Cam. But I have some issu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchcam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SSCAM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m image_data_module\u001b[38;5;241m.\u001b[39mtrain_dataset:\n\u001b[0;32m----> 4\u001b[0m     cam \u001b[38;5;241m=\u001b[39m SSCAM(classifier)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[1;32m      6\u001b[0m         out \u001b[38;5;241m=\u001b[39m classifier(input_tensor)\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/activation.py:292\u001b[0m, in \u001b[0;36mSSCAM.__init__\u001b[0;34m(self, model, target_layer, batch_size, num_samples, std, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    283\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    290\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m=\u001b[39m num_samples\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd \u001b[38;5;241m=\u001b[39m std\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/activation.py:147\u001b[0m, in \u001b[0;36mScoreCAM.__init__\u001b[0;34m(self, model, target_layer, batch_size, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Input hook\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_handles\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mregister_forward_pre_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_input))\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/core.py:53\u001b[0m, in \u001b[0;36m_CAM.__init__\u001b[0;34m(self, model, target_layer, input_shape, enable_hooks)\u001b[0m\n\u001b[1;32m     48\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_layer_name(layer) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m target_layer\n\u001b[1;32m     50\u001b[0m     ]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m target_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# If the layer is not specified, try automatic resolution\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     target_name \u001b[38;5;241m=\u001b[39m \u001b[43mlocate_candidate_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Warn the user of the choice\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_name, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/_utils.py:43\u001b[0m, in \u001b[0;36mlocate_candidate_layer\u001b[0;34m(mod, input_shape)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# forward empty\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Remove all temporary hooks\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m hook_handles:\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/fight_classifier/fight_classifier/model/image_based_model.py:110\u001b[0m, in \u001b[0;36mProjFromFeatures.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 110\u001b[0m     base_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflatten\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    111\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_layer(base_features)\n\u001b[1;32m    112\u001b[0m     probas \u001b[38;5;241m=\u001b[39m softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/fx/graph_module.py:652\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/fx/graph_module.py:277\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/fx/graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1151\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1151\u001b[0m         hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1153\u001b[0m             result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/_utils.py:34\u001b[0m, in \u001b[0;36mlocate_candidate_layer.<locals>._record_output_shape\u001b[0;34m(module, input, output, name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_record_output_shape\u001b[39m(module: nn\u001b[38;5;241m.\u001b[39mModule, \u001b[38;5;28minput\u001b[39m: Tensor, output: Tensor, name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124;03m\"\"\"Activation hook.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     output_shapes\u001b[38;5;241m.\u001b[39mappend((name, \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchcam.methods import SSCAM\n",
    "\n",
    "for example in image_data_module.train_dataset:\n",
    "    cam = SSCAM(classifier)\n",
    "    with torch.no_grad(): \n",
    "        out = classifier(input_tensor)\n",
    "    cam(class_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aviva",
   "language": "python",
   "name": "aviva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
