{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-trivial model\n",
    "\n",
    "Finally, we make an attempt at classifying videos.\n",
    "\n",
    "We train an image classifier. Once it is trained, we will classify videos this way: classify every frame of the video, and average the probas to get the classification probabilities for the whole video.\n",
    "\n",
    "* This has the advantage of giving us ~10k (input, groundtruth) pairs, compared to ~200 for an end-to-end video classifier.\n",
    "* Image classifiers are easier to visualize and debug.\n",
    "* There is more research being done on images, which leads to better architectures/models despite the theoretical disadvantage of the limitation. For instance, __[the CoCa video classifier](https://paperswithcode.com/paper/coca-contrastive-captioners-are-image-text)__ (which is based on an image-model, with late fusion of information) is SOTA for some video datasets, and among the best models for others.\n",
    "* It is easier to use the image models, since they are packaged with the main deep learning frameworks.\n",
    "\n",
    "My initial plan was to use this as a baseline and then train a linear layer on top of a __[MTV model](https://paperswithcode.com/paper/multiview-transformers-for-video-recognition)__ . However, the problem I got with the image-based classifier was not that its accuracy was too low. It was suspiciously too high. This pushed me to try fixing the dataset either. But, even after:\n",
    "* Writing a resizing transformation which does not show the difference in height between fights and non-fight.\n",
    "* Applying an extreme color augmentation until tiny models do not do better than chance.\n",
    "* Manually annotating videos to avoid correlations between train and validation datasets.\n",
    "\n",
    "The validation accuracy we get is close to 100%. There are probably other Clever Hans effects we did not get rid off. But, at this point, it is outside the scope of this challenge. We would recommend spending more time on aquiring more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from fight_classifier import DATASET_DIR, PROJECT_DIR\n",
    "\n",
    "frames_dir = DATASET_DIR / 'raw_frames/'\n",
    "videos_dir = DATASET_DIR / 'Peliculas/'\n",
    "\n",
    "videos_df = pd.read_csv(videos_dir / 'videos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthieu/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/matthieu/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 803: system has unsupported display driver / cuda driver combination (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home/matthieu/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageClassifierModule(\n",
       "  (classifier): ProjFromFeatures(\n",
       "    (base_model): MobileNetV3(\n",
       "      (features): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (12): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (15): InvertedResidual(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (16): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (classifier): Sequential(\n",
       "        (0): Linear(in_features=960, out_features=1280, bias=True)\n",
       "        (1): Hardswish()\n",
       "        (2): Dropout(p=0.2, inplace=True)\n",
       "        (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (proj_layer): Linear(in_features=960, out_features=2, bias=True)\n",
       "    (feature_extractor): MobileNetV3(\n",
       "      (features): Module(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "              (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "              (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "              (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "              (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): Conv2dNormActivation(\n",
       "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "              (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (12): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (13): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "              (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (14): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (15): Module(\n",
       "          (block): Module(\n",
       "            (0): Conv2dNormActivation(\n",
       "              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (1): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              (2): Hardswish()\n",
       "            )\n",
       "            (2): SqueezeExcitation(\n",
       "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "              (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (activation): ReLU()\n",
       "              (scale_activation): Hardsigmoid()\n",
       "            )\n",
       "            (3): Conv2dNormActivation(\n",
       "              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (16): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "  )\n",
       "  (train_precision): Precision()\n",
       "  (train_recall): Recall()\n",
       "  (train_f1): F1Score()\n",
       "  (train_pr_curve): PrecisionRecallCurve()\n",
       "  (train_confusion_matrix): ConfusionMatrix()\n",
       "  (val_precision): Precision()\n",
       "  (val_recall): Recall()\n",
       "  (val_f1): F1Score()\n",
       "  (val_pr_curve): PrecisionRecallCurve()\n",
       "  (val_confusion_matrix): ConfusionMatrix()\n",
       "  (test_precision): Precision()\n",
       "  (test_recall): Recall()\n",
       "  (test_f1): F1Score()\n",
       "  (test_pr_curve): PrecisionRecallCurve()\n",
       "  (test_confusion_matrix): ConfusionMatrix()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchvision.models import MobileNet_V3_Large_Weights\n",
    "\n",
    "from fight_classifier.data.image_dataset import ImageDataModule\n",
    "from fight_classifier.model.image_based_model import \\\n",
    "    ProjFromFeatures\n",
    "from fight_classifier.torch_module.image_classifier import \\\n",
    "    ImageClassifierModule\n",
    "\n",
    "BATCH_SIZE = 40\n",
    "SPLIT_COHERENCE_COL = 'fine_category'\n",
    "\n",
    "frames_df = pd.read_csv(str(frames_dir / 'frames.csv'))\n",
    "\n",
    "# We base our pre-processing on Mobilenet because it's the\n",
    "# base of the model we will train in the next notebook.\n",
    "base_model_weights = MobileNet_V3_Large_Weights.DEFAULT\n",
    "preprocess = base_model_weights.transforms()\n",
    "preprocess_kwargs = {\n",
    "    'resize_size': preprocess.resize_size[0],\n",
    "    'crop_size': preprocess.crop_size[0],\n",
    "    'mean': preprocess.mean,\n",
    "    'std': preprocess.std,\n",
    "}\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=str(PROJECT_DIR),\n",
    "    max_epochs=10,\n",
    ")\n",
    "\n",
    "\n",
    "image_data_module = ImageDataModule(\n",
    "    image_df=frames_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    preprocess_kwargs=preprocess_kwargs,\n",
    "    split_coherence_col=SPLIT_COHERENCE_COL,\n",
    "    image_augmentation=False)\n",
    "image_data_module.setup()\n",
    "classifier = ProjFromFeatures(n_classes=2)\n",
    "classif_module = ImageClassifierModule(classifier=classifier)\n",
    "\n",
    "# Instead of training the model, we can load it from the checkpoint\n",
    "\n",
    "# trainer.fit(\n",
    "#    model=classif_module,\n",
    "#    datamodule=image_data_module)\n",
    "\n",
    "classif_module.load_from_checkpoint(\n",
    "    str(PROJECT_DIR / 'checkpoints/epoch=9-step=2110.ckpt'),\n",
    "    classifier=classifier,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From images to videos\n",
    "\n",
    "We use `ImageBasedVideoClassifier` to classify videos: we run the image classifier on every other image (to save computation time) and average them to get the video classification. Logically, since the image classifier's validation performance is almost perfect, the video classification is also almost perfect on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███████████████████████████████████████████████████████▎                                                                                                             | 66/197 [02:13<04:06,  1.88s/it]"
     ]
    }
   ],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import skvideo.io\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from fight_classifier.model.image_based_model import ImageBasedVideoClassifier\n",
    "\n",
    "classifier.eval()\n",
    "video_classifier = ImageBasedVideoClassifier(\n",
    "    image_classifier=classifier)\n",
    "\n",
    "predictions_probas = []\n",
    "predictions = []\n",
    "groundtruths = []\n",
    "\n",
    "\n",
    "for _, video_row in tqdm.tqdm(videos_df.iterrows(), total=len(videos_df)):\n",
    "    video_path = str(DATASET_DIR / video_row.video_path)\n",
    "    video_fhwc = skvideo.io.vread(video_path)\n",
    "    with torch.no_grad():\n",
    "        frames_bhwc = torch.Tensor([\n",
    "            image_data_module.train_dataset.preprocess_image(image_np)[1]\n",
    "            for image_np in video_fhwc\n",
    "        ])\n",
    "        frames_bchw = torch.permute(frames_bhwc, (0, 3, 1, 2))\n",
    "        video_proba = video_classifier(frames_bchw[None]).numpy()\n",
    "        groundtruths.append(video_row.is_fight)\n",
    "        predictions_probas.append(video_proba)\n",
    "        predictions.append(np.argmax(video_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of video classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.98\n",
      "Recall = 1.00\n",
      "F1-score = 0.99\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fight_classifier.visualization import imshow_chw\n",
    "from fight_classifier.visualization.metrics import plot_confusion_matrix\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(\n",
    "    y_true=groundtruths, y_pred=predictions)\n",
    "precision = confusion_matrix[1,1] / confusion_matrix[:,1].sum()\n",
    "recall = confusion_matrix[1,1] / confusion_matrix[1,:].sum()\n",
    "f1_score = 2 * precision * recall / (precision+recall)\n",
    "print(\n",
    "    f\"Precision = {precision:.2f}\\n\"\n",
    "    f\"Recall = {recall:.2f}\\n\"\n",
    "    f\"F1-score = {f1_score:.2f}\\n\"\n",
    ")\n",
    "cm_viz_chw = plot_confusion_matrix(\n",
    "    cm=confusion_matrix, class_names=['no_fight', 'fight'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_axis_off()\n",
    "imshow_chw(img_chw=cm_viz_chw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation map\n",
    "\n",
    "I thought I would show some activation maps of the image model, thanks to Torch Cam. But I have some issues that I did not manage to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchcam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SSCAM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m image_data_module\u001b[38;5;241m.\u001b[39mtrain_dataset:\n\u001b[0;32m----> 4\u001b[0m     cam \u001b[38;5;241m=\u001b[39m SSCAM(classifier)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[1;32m      6\u001b[0m         out \u001b[38;5;241m=\u001b[39m classifier(input_tensor)\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/activation.py:292\u001b[0m, in \u001b[0;36mSSCAM.__init__\u001b[0;34m(self, model, target_layer, batch_size, num_samples, std, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    283\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    290\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m=\u001b[39m num_samples\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd \u001b[38;5;241m=\u001b[39m std\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/activation.py:147\u001b[0m, in \u001b[0;36mScoreCAM.__init__\u001b[0;34m(self, model, target_layer, batch_size, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    145\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# Input hook\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_handles\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mregister_forward_pre_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_input))\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/core.py:53\u001b[0m, in \u001b[0;36m_CAM.__init__\u001b[0;34m(self, model, target_layer, input_shape, enable_hooks)\u001b[0m\n\u001b[1;32m     48\u001b[0m     target_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_layer_name(layer) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;28;01melse\u001b[39;00m layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m target_layer\n\u001b[1;32m     50\u001b[0m     ]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m target_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# If the layer is not specified, try automatic resolution\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     target_name \u001b[38;5;241m=\u001b[39m \u001b[43mlocate_candidate_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Warn the user of the choice\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_name, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/_utils.py:43\u001b[0m, in \u001b[0;36mlocate_candidate_layer\u001b[0;34m(mod, input_shape)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# forward empty\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Remove all temporary hooks\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m hook_handles:\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/fight_classifier/fight_classifier/model/image_based_model.py:110\u001b[0m, in \u001b[0;36mProjFromFeatures.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 110\u001b[0m     base_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflatten\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    111\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_layer(base_features)\n\u001b[1;32m    112\u001b[0m     probas \u001b[38;5;241m=\u001b[39m softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/fx/graph_module.py:652\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/fx/graph_module.py:277\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/fx/graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1151\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m-> 1151\u001b[0m         hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1153\u001b[0m             result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/torchcam/methods/_utils.py:34\u001b[0m, in \u001b[0;36mlocate_candidate_layer.<locals>._record_output_shape\u001b[0;34m(module, input, output, name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_record_output_shape\u001b[39m(module: nn\u001b[38;5;241m.\u001b[39mModule, \u001b[38;5;28minput\u001b[39m: Tensor, output: Tensor, name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124;03m\"\"\"Activation hook.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     output_shapes\u001b[38;5;241m.\u001b[39mappend((name, \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchcam.methods import SSCAM\n",
    "\n",
    "for example in image_data_module.train_dataset:\n",
    "    cam = SSCAM(classifier)\n",
    "    with torch.no_grad(): \n",
    "        out = classifier(input_tensor)\n",
    "    cam(class_idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure to save with joblib\n",
    "\n",
    "Another failure: I tried to save the model with joblib but could not debug it. I am unfamiliar with joblib and I am not sure it is the best way to export a torch model anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function locate_candidate_layer.<locals>._record_output_shape at 0x7fd950f5ab90>: it's not found as torchcam.methods._utils.locate_candidate_layer.<locals>._record_output_shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfight_classifier\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PROJECT_DIR\n\u001b[1;32m      4\u001b[0m joblib_dump_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(PROJECT_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoblib_image_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(value\u001b[38;5;241m=\u001b[39mclassifier, filename\u001b[38;5;241m=\u001b[39mjoblib_dump_path)\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/joblib/numpy_pickle.py:553\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 553\u001b[0m         \u001b[43mNumpyPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     NumpyPickler(filename, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:487\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mstart_framing()\n\u001b[0;32m--> 487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(STOP)\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mend_framing()\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/joblib/numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:717\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m         write(BUILD)\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    723\u001b[0m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/joblib/numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:972\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 972\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_setitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:998\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[1;32m    997\u001b[0m         save(k)\n\u001b[0;32m--> 998\u001b[0m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/joblib/numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:713\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_appends(listitems)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dictitems \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_setitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1003\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     k, v \u001b[38;5;241m=\u001b[39m tmp[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1002\u001b[0m     save(k)\n\u001b[0;32m-> 1003\u001b[0m     \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1004\u001b[0m     write(SETITEM)\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;66;03m# else tmp is empty, and we're done\u001b[39;00m\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/joblib/numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:603\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:692\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     save(func)\n\u001b[0;32m--> 692\u001b[0m     \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m     write(REDUCE)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# If the object is already in the memo, this means it is\u001b[39;00m\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;66;03m# recursive. In this case, throw away everything we put on the\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;66;03m# stack, and fetch the object back from the memo.\u001b[39;00m\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/joblib/numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:887\u001b[0m, in \u001b[0;36m_Pickler.save_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m obj:\n\u001b[0;32m--> 887\u001b[0m         \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# Subtle.  Same as in the big comment below.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(obj) \u001b[38;5;129;01min\u001b[39;00m memo:\n",
      "File \u001b[0;32m~/aviva/hiring-challenge/venv/lib/python3.10/site-packages/joblib/numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:560\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    558\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/pickle.py:1071\u001b[0m, in \u001b[0;36m_Pickler.save_global\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     obj2, parent \u001b[38;5;241m=\u001b[39m _getattribute(module, name)\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[0;32m-> 1071\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\n\u001b[1;32m   1072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m: it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not found as \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1073\u001b[0m         (obj, module_name, name)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj:\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function locate_candidate_layer.<locals>._record_output_shape at 0x7fd950f5ab90>: it's not found as torchcam.methods._utils.locate_candidate_layer.<locals>._record_output_shape"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from fight_classifier import PROJECT_DIR\n",
    "\n",
    "joblib_dump_path = str(PROJECT_DIR / 'joblib_image_classifier')\n",
    "joblib.dump(value=classifier, filename=joblib_dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aviva",
   "language": "python",
   "name": "aviva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
